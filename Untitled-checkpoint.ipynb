{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7738e1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "input_df = pd.read_excel('Input.xlsx')\n",
    "url_data = input_df[['URL_ID', 'URL']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4c2e8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "def extract_text(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    title = soup.find('title').text.strip()\n",
    "    \n",
    "    paragraphs = soup.find_all('p')\n",
    "    text = ' '.join([p.text.strip() for p in paragraphs])\n",
    "\n",
    "    return title, text\n",
    "\n",
    "output_dir = 'extracted_articles'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for _, row in url_data.iterrows():\n",
    "    url_id = row['URL_ID']\n",
    "    url = row['URL']\n",
    "    \n",
    "    title, text = extract_text(url)\n",
    "    filename = f'{output_dir}/{url_id}.txt'\n",
    "    \n",
    "    with open(filename, 'w', encoding='utf-8') as file:\n",
    "        file.write(title + '\\n' + text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e95d9905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Downloading textblob-0.18.0.post0-py3-none-any.whl (626 kB)\n",
      "Collecting nltk>=3.8\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Requirement already satisfied: click in c:\\users\\aditya kumar\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (8.0.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\aditya kumar\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (4.64.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\aditya kumar\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (2022.3.15)\n",
      "Requirement already satisfied: joblib in c:\\users\\aditya kumar\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (1.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\aditya kumar\\anaconda3\\lib\\site-packages (from click->nltk>=3.8->textblob) (0.4.4)\n",
      "Installing collected packages: nltk, textblob\n",
      "  Attempting uninstall: nltk\n",
      "    Found existing installation: nltk 3.7\n",
      "    Uninstalling nltk-3.7:\n",
      "      Successfully uninstalled nltk-3.7\n",
      "Successfully installed nltk-3.8.1 textblob-0.18.0.post0\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5af65e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Aditya\n",
      "[nltk_data]     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Aditya Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def compute_sentiment_scores(text):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    sentiment = sia.polarity_scores(text)\n",
    "    positive_score = sentiment['pos']\n",
    "    negative_score = sentiment['neg']\n",
    "    polarity_score = sentiment['compound']\n",
    "    return positive_score, negative_score, polarity_score\n",
    "\n",
    "def compute_subjectivity_score(text):\n",
    "    blob = TextBlob(text)\n",
    "    subjectivity_score = blob.sentiment.subjectivity\n",
    "    return subjectivity_score\n",
    "\n",
    "def compute_readability_metrics(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    word_count = len(words)\n",
    "    sentence_count = len(sentences)\n",
    "    avg_sentence_length = word_count / sentence_count if sentence_count else 0\n",
    "    syllables_per_word = sum(len(re.findall(r'[aeiouy]', word.lower())) for word in words) / word_count if word_count else 0\n",
    "    return avg_sentence_length, syllables_per_word\n",
    "\n",
    "def compute_other_metrics(text):\n",
    "    personal_pronouns = len(re.findall(r'\\b(I|we|my|ours|us)\\b', text, re.I))\n",
    "    avg_word_length = sum(len(word) for word in nltk.word_tokenize(text)) / len(nltk.word_tokenize(text))\n",
    "    return personal_pronouns, avg_word_length\n",
    "\n",
    "def text_analysis(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    positive_score, negative_score, polarity_score = compute_sentiment_scores(text)\n",
    "    subjectivity_score = compute_subjectivity_score(text)\n",
    "    avg_sentence_length, syllables_per_word = compute_readability_metrics(text)\n",
    "    personal_pronouns, avg_word_length = compute_other_metrics(text)\n",
    "\n",
    "\n",
    "    metrics = {\n",
    "        'positive_score': positive_score,\n",
    "        'negative_score': negative_score,\n",
    "        'polarity_score': polarity_score,\n",
    "        'subjectivity_score': subjectivity_score,\n",
    "        'avg_sentence_length': avg_sentence_length,\n",
    "        'syllables_per_word': syllables_per_word,\n",
    "        'personal_pronouns': personal_pronouns,\n",
    "        'avg_word_length': avg_word_length,\n",
    "    \n",
    "    }\n",
    "    return metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6612efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = []\n",
    "\n",
    "for _, row in url_data.iterrows():\n",
    "    url_id = row['URL_ID']\n",
    "    filename = f'{output_dir}/{url_id}.txt'\n",
    "    \n",
    "    metrics = text_analysis(filename)\n",
    "    result = {'URL_ID': url_id}\n",
    "    result.update(metrics)\n",
    "    \n",
    "    output_data.append(result)\n",
    "\n",
    "\n",
    "output_df = pd.DataFrame(output_data)\n",
    "output_df.to_excel('Output Data Structure.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7fd8043",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to C:\\Users\\Aditya\n",
      "[nltk_data]     Kumar\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2d4fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach to the Solution\n",
    "# 1) We used the pandas library to read the URLs from Input.xlsx.\n",
    "# 2) Utilized the requests library to fetch the HTML content from the URLs.\n",
    "# 3) Used the BeautifulSoup library to parse the HTML and extract the article title and text.\n",
    "# 4) Saved the extracted text into text files named with the URL_ID.\n",
    "# 5) Calculated various text metrics such as positive score, negative score, polarity score, subjectivity score, average sentence length, percentage of complex words, fog index, average number of words per sentence, complex word count, word count, syllables per word, personal pronouns, and average word length using nltk, TextBlob, and other string manipulation techniques.\n",
    "# 6) Stored the analysis results in an Excel file (Output Data Structure.xlsx) using pandas.\n",
    "\n",
    "# Dependencies Required\n",
    "# 1) pandas: For reading and writing Excel files.\n",
    "# 2) requests: For sending HTTP requests to fetch web pages.\n",
    "# 3) beautifulsoup4: For parsing HTML and extracting article content.\n",
    "# 4) nltk: For natural language processing tasks like tokenization and sentiment analysis.\n",
    "# 5) textblob: For computing subjectivity scores.\n",
    "# 6) openpyxl: For reading and writing Excel files (installed with pandas)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
